{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e723dc",
   "metadata": {},
   "source": [
    "# Model MT5\n",
    "attention based model\n",
    "text -> Model -> new text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "245588b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import os\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "750f3d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['europarl-v7.cs-en.cs',\n",
       " 'europarl-v7.cs-en.en',\n",
       " 'europarl-v7.de-en.de',\n",
       " 'europarl-v7.de-en.en',\n",
       " 'europarl-v7.es-en.en',\n",
       " 'europarl-v7.es-en.es',\n",
       " 'europarl-v7.fr-en.en',\n",
       " 'europarl-v7.fr-en.fr',\n",
       " 'questions_easy.csv']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'training'\n",
    "os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "eb508a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'/europarl-v7.de-en.en',encoding='utf-8') as f: en = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "644a93dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'/europarl-v7.de-en.de',encoding='utf-8') as f: de = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4f8e7c4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Resumption of the session'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3c62e1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wiederaufnahme der Sitzungsperiode'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "de[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "df2ba6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4f5f12f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_eq = re.compile('^(Wh[^?.!]+\\?)')\n",
    "re_dq = re.compile('^([^?.!]+\\?)')\n",
    "en_fname = path+'/europarl-v7.de-en.en'\n",
    "de_fname = path+'/europarl-v7.de-en.de'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "34e7a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = ((eq,fq)\n",
    "for eq, fq in zip(open(en_fname, encoding='utf-8'), open(de_fname, encoding='utf-8')))\n",
    "qs = [(e, d) for e,d in lines if e and d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0d9ff7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = qs[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c70970c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6aa5b992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b804734d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.array(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5cfa2c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Resumption of the session\\n',\n",
       "       'I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\\n',\n",
       "       \"Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\\n\",\n",
       "       ...,\n",
       "       'Freight is growing out of all proportion too, and is expected to increase by 70% over the next ten years.\\n',\n",
       "       'It is surprising that there is no breakthrough in intermodal transport in the offing.\\n',\n",
       "       'Perhaps the pressure on the transporters and the shippers is not yet so great as to bring about this breakthrough.\\n'],\n",
       "      dtype='<U1872')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0268cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "431d16f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mark_start = 'ssss '\n",
    "mark_end = ' eeee'\n",
    "for index, word in enumerate(qs[:,0]):\n",
    "    qs[:,0][index] = word[:-1]\n",
    "    #qs[:,0][index] = mark_start + word + mark_end\n",
    "for index, word in enumerate(qs[:,1]):\n",
    "    qs[:,1][index] = word[:-1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "fffb5d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = [(q1,q2) for q1,q2 in qs]\n",
    "df = pd.DataFrame({'de': [q[1] for q in qs], 'en': [q[0] for q in qs]}, columns = ['en', 'de'])\n",
    "df.to_csv(path+'/questions_easy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ac85580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.array(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "919f2542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.',\n",
       "       'Ich erkläre die am Freitag, dem 17. Dezember unterbrochene Sitzungsperiode des Europäischen Parlaments für wiederaufgenommen, wünsche Ihnen nochmals alles Gute zum Jahreswechsel und hoffe, daß Sie schöne Ferien hatten.'],\n",
       "      dtype='<U1871')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9310a071",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, word in enumerate(qs[:,0]):\n",
    "    #qs[:,0][index] = word[:-1]\n",
    "    qs[:,0][index] = mark_start + word + mark_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5d7bf950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ssss Resumption of the session eeee',\n",
       "       'Wiederaufnahme der Sitzungsperiode'], dtype='<U1871')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "66ba8565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Resumption of the session</td>\n",
       "      <td>Wiederaufnahme der Sitzungsperiode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I declare resumed the session of the European ...</td>\n",
       "      <td>Ich erkläre die am Freitag, dem 17. Dezember u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Although, as you will have seen, the dreaded '...</td>\n",
       "      <td>Wie Sie feststellen konnten, ist der gefürchte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You have requested a debate on this subject in...</td>\n",
       "      <td>Im Parlament besteht der Wunsch nach einer Aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In the meantime, I should like to observe a mi...</td>\n",
       "      <td>Heute möchte ich Sie bitten - das ist auch der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                          Resumption of the session   \n",
       "1  I declare resumed the session of the European ...   \n",
       "2  Although, as you will have seen, the dreaded '...   \n",
       "3  You have requested a debate on this subject in...   \n",
       "4  In the meantime, I should like to observe a mi...   \n",
       "\n",
       "                                                  de  \n",
       "0                 Wiederaufnahme der Sitzungsperiode  \n",
       "1  Ich erkläre die am Freitag, dem 17. Dezember u...  \n",
       "2  Wie Sie feststellen konnten, ist der gefürchte...  \n",
       "3  Im Parlament besteht der Wunsch nach einer Aus...  \n",
       "4  Heute möchte ich Sie bitten - das ist auch der...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(path+'/questions_easy.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11320862",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f6be65db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "en    object\n",
       "de    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "20410753",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d5f4afb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['en'] = df['en'].apply(lambda x:x.lower())\n",
    "df['de'] = df['de'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079b59d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ac32e00c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>en</th>\n",
       "      <th>de</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>resumption of the session</td>\n",
       "      <td>wiederaufnahme der sitzungsperiode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i declare resumed the session of the european ...</td>\n",
       "      <td>ich erkläre die am freitag, dem 17. dezember u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>although, as you will have seen, the dreaded '...</td>\n",
       "      <td>wie sie feststellen konnten, ist der gefürchte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you have requested a debate on this subject in...</td>\n",
       "      <td>im parlament besteht der wunsch nach einer aus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in the meantime, i should like to observe a mi...</td>\n",
       "      <td>heute möchte ich sie bitten - das ist auch der...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  en  \\\n",
       "0                          resumption of the session   \n",
       "1  i declare resumed the session of the european ...   \n",
       "2  although, as you will have seen, the dreaded '...   \n",
       "3  you have requested a debate on this subject in...   \n",
       "4  in the meantime, i should like to observe a mi...   \n",
       "\n",
       "                                                  de  \n",
       "0                 wiederaufnahme der sitzungsperiode  \n",
       "1  ich erkläre die am freitag, dem 17. dezember u...  \n",
       "2  wie sie feststellen konnten, ist der gefürchte...  \n",
       "3  im parlament besteht der wunsch nach einer aus...  \n",
       "4  heute möchte ich sie bitten - das ist auch der...  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "0f418970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "87261f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ffbe7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5f8e6913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "class TokenizerWrap(Tokenizer):\n",
    "    \"\"\"Wrap the Tokenizer-class from Keras with more functionality.\"\"\"\n",
    "    \n",
    "    def __init__(self, texts, padding,\n",
    "                 reverse=False, num_words=None):\n",
    "        \"\"\"\n",
    "        :param texts: List of strings. This is the data-set.\n",
    "        :param padding: Either 'post' or 'pre' padding.\n",
    "        :param reverse: Boolean whether to reverse token-lists.\n",
    "        :param num_words: Max number of words to use.\n",
    "        \"\"\"\n",
    "\n",
    "        Tokenizer.__init__(self, num_words=num_words)\n",
    "\n",
    "        # Create the vocabulary from the texts.\n",
    "        self.fit_on_texts(texts)\n",
    "\n",
    "        # Create inverse lookup from integer-tokens to words.\n",
    "        self.index_to_word = dict(zip(self.word_index.values(),\n",
    "                                      self.word_index.keys()))\n",
    "\n",
    "        # Convert all texts to lists of integer-tokens.\n",
    "        # Note that the sequences may have different lengths.\n",
    "        self.tokens = self.texts_to_sequences(texts)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the token-sequences.\n",
    "            self.tokens = [list(reversed(x)) for x in self.tokens]\n",
    "        \n",
    "            # Sequences that are too long should now be truncated\n",
    "            # at the beginning, which corresponds to the end of\n",
    "            # the original sequences.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        # The number of integer-tokens in each sequence.\n",
    "        self.num_tokens = [len(x) for x in self.tokens]\n",
    "\n",
    "        # Max number of tokens to use in all sequences.\n",
    "        # We will pad / truncate all sequences to this length.\n",
    "        # This is a compromise so we save a lot of memory and\n",
    "        # only have to truncate maybe 5% of all the sequences.\n",
    "        self.max_tokens = np.mean(self.num_tokens) \\\n",
    "                          + 2 * np.std(self.num_tokens)\n",
    "        self.max_tokens = int(self.max_tokens)\n",
    "\n",
    "        # Pad / truncate all token-sequences to the given length.\n",
    "        # This creates a 2-dim numpy matrix that is easier to use.\n",
    "        self.tokens_padded = pad_sequences(self.tokens,\n",
    "                                           maxlen=self.max_tokens,\n",
    "                                           padding=padding,\n",
    "                                           truncating=truncating)\n",
    "\n",
    "    def token_to_word(self, token):\n",
    "        \"\"\"Lookup a single word from an integer-token.\"\"\"\n",
    "\n",
    "        word = \" \" if token == 0 else self.index_to_word[token]\n",
    "        return word \n",
    "\n",
    "    def tokens_to_string(self, tokens):\n",
    "        \"\"\"Convert a list of integer-tokens to a string.\"\"\"\n",
    "\n",
    "        # Create a list of the individual words.\n",
    "        words = [self.index_to_word[token]\n",
    "                 for token in tokens\n",
    "                 if token != 0]\n",
    "        \n",
    "        # Concatenate the words to a single string\n",
    "        # with space between all the words.\n",
    "        text = \" \".join(words)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def text_to_tokens(self, text, reverse=False, padding=False):\n",
    "        \"\"\"\n",
    "        Convert a single text-string to tokens with optional\n",
    "        reversal and padding.\n",
    "        \"\"\"\n",
    "\n",
    "        # Convert to tokens. Note that we assume there is only\n",
    "        # a single text-string so we wrap it in a list.\n",
    "        tokens = self.texts_to_sequences([text])\n",
    "        tokens = np.array(tokens)\n",
    "\n",
    "        if reverse:\n",
    "            # Reverse the tokens.\n",
    "            tokens = np.flip(tokens, axis=1)\n",
    "\n",
    "            # Sequences that are too long should now be truncated\n",
    "            # at the beginning, which corresponds to the end of\n",
    "            # the original sequences.\n",
    "            truncating = 'pre'\n",
    "        else:\n",
    "            # Sequences that are too long should be truncated\n",
    "            # at the end.\n",
    "            truncating = 'post'\n",
    "\n",
    "        if padding:\n",
    "            # Pad and truncate sequences to the given length.\n",
    "            tokens = pad_sequences(tokens,\n",
    "                                   maxlen=self.max_tokens,\n",
    "                                   padding='pre',\n",
    "                                   truncating=truncating)\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad99dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d5122c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words =10000\n",
    "tokenizer_src = TokenizerWrap(texts=qs[:,0],\n",
    "                              padding='pre',\n",
    "                              reverse=True,\n",
    "                              num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7d9f48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_dest = TokenizerWrap(texts=qs[:,1],\n",
    "                               padding='post',\n",
    "                               reverse=False,\n",
    "                               num_words=num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1fe6e2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 57)\n",
      "(100000, 48)\n"
     ]
    }
   ],
   "source": [
    "tokens_src = tokenizer_src.tokens_padded\n",
    "tokens_dest = tokenizer_dest.tokens_padded\n",
    "print(tokens_src.shape)\n",
    "print(tokens_dest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bfeb7b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    3,  926,    1,    4,\n",
       "       5176,    2])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_src[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e95ddfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_start = tokenizer_src.word_index[mark_end.strip()]\n",
    "token_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dc0bb7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "978"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_end = tokenizer_dest.word_index['weshalb']\n",
    "token_end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "03f67060",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "20cd08e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    3, 4987,  106, 1610,   10, 1823,\n",
       "        954,    4, 1450,    9, 2139,   68,    4,  207,    9,    7,   82,\n",
       "          1,  180, 7944,    5, 1710,    1,  639,   22,   26,   44,   20,\n",
       "        362,    2])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_src[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6e9ab6d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eeee dreadful were truly that disasters natural of series a suffered countries of number a in people the still materialise to failed the seen have will you as although ssss'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_src.tokens_to_string(tokens_src[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01f48d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ssss Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful. eeee\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[:,0][idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4256dfcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 47)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data = tokens_src\n",
    "decoder_input_data = tokens_dest[:, :-1]\n",
    "decoder_output_data = tokens_dest[:, 1:]\n",
    "decoder_output_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "7206fac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  30,   23,  755, 1034,   12,    2,   14, 4648,  100,   28,  150,\n",
       "        923,  123,   76,  634,   10, 4326, 4250,  955,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_dest[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "720210c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wie sie feststellen konnten ist der nicht eingetreten doch sind bürger einiger unserer mitgliedstaaten opfer von schrecklichen naturkatastrophen geworden'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_dest.tokens_to_string(tokens_dest[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c84aa117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Wie Sie feststellen konnten, ist der gefürchtete \"Millenium-Bug \" nicht eingetreten. Doch sind Bürger einiger unserer Mitgliedstaaten Opfer von schrecklichen Naturkatastrophen geworden.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qs[:,1][idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ddc604e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "b69378ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = Input(shape=(None, ), name='encoder_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "a7dc7dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "03b938dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='encoder_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f3390452",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "397ed797",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_gru1 = GRU(state_size, name='encoder_gru1',\n",
    "                   return_sequences=True)\n",
    "encoder_gru2 = GRU(state_size, name='encoder_gru2',\n",
    "                   return_sequences=True)\n",
    "encoder_gru3 = GRU(state_size, name='encoder_gru3',\n",
    "                   return_sequences=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7092a7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_encoder():\n",
    "    # Start the neural network with its input-layer.\n",
    "    net = encoder_input\n",
    "    \n",
    "    # Connect the embedding-layer.\n",
    "    net = encoder_embedding(net)\n",
    "\n",
    "    # Connect all the GRU-layers.\n",
    "    net = encoder_gru1(net)\n",
    "    net = encoder_gru2(net)\n",
    "    net = encoder_gru3(net)\n",
    "\n",
    "    # This is the output of the encoder.\n",
    "    encoder_output = net\n",
    "    \n",
    "    return encoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "5db5ade1",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_output = connect_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "19a31372",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_initial_state = Input(shape=(state_size,),\n",
    "                              name='decoder_initial_state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "52d74408",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(None, ), name='decoder_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4062f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_embedding = Embedding(input_dim=num_words,\n",
    "                              output_dim=embedding_size,\n",
    "                              name='decoder_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "3f239429",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_gru1 = GRU(state_size, name='decoder_gru1',\n",
    "                   return_sequences=True)\n",
    "decoder_gru2 = GRU(state_size, name='decoder_gru2',\n",
    "                   return_sequences=True)\n",
    "decoder_gru3 = GRU(state_size, name='decoder_gru3',\n",
    "                   return_sequences=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fae7e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense = Dense(num_words,\n",
    "                      activation='softmax',\n",
    "                      name='decoder_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8542b4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_decoder(initial_state):\n",
    "    # Start the decoder-network with its input-layer.\n",
    "    net = decoder_input\n",
    "\n",
    "    # Connect the embedding-layer.\n",
    "    net = decoder_embedding(net)\n",
    "    \n",
    "    # Connect all the GRU-layers.\n",
    "    net = decoder_gru1(net, initial_state=initial_state)\n",
    "    net = decoder_gru2(net, initial_state=initial_state)\n",
    "    net = decoder_gru3(net, initial_state=initial_state)\n",
    "\n",
    "    # Connect the final dense layer that converts to\n",
    "    # one-hot encoded arrays.\n",
    "    decoder_output = decoder_dense(net)\n",
    "    \n",
    "    return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "09d9653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0d49db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=encoder_output)\n",
    "\n",
    "model_train = Model(inputs=[encoder_input, decoder_input],\n",
    "                    outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "6534c684",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_encoder = Model(inputs=[encoder_input],\n",
    "                      outputs=[encoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "14df8e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = connect_decoder(initial_state=decoder_initial_state)\n",
    "\n",
    "model_decoder = Model(inputs=[decoder_input, decoder_initial_state],\n",
    "                      outputs=[decoder_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "336c9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4633a97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_train.compile(optimizer=RMSprop(learning_rate=1e-3),\n",
    "                    loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6fe598ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d112cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_checkpoint = '21_checkpoint.keras'\n",
    "callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint,\n",
    "                                      monitor='val_loss',\n",
    "                                      verbose=1,\n",
    "                                      save_weights_only=True,\n",
    "                                      save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a947aace",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                        patience=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "492b677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback_tensorboard = TensorBoard(log_dir='./21_logs/',\n",
    "                                   histogram_freq=0,\n",
    "                                   write_graph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3960fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [callback_early_stopping,\n",
    "             callback_checkpoint,\n",
    "             callback_tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "22397385",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model_train.load_weights(path_checkpoint)\n",
    "except Exception as error:\n",
    "    print(\"Error trying to load checkpoint.\")\n",
    "    print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "3a4d1fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = \\\n",
    "{\n",
    "    'encoder_input': encoder_input_data,\n",
    "    'decoder_input': decoder_input_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "53126ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = \\\n",
    "{\n",
    "    'decoder_output': decoder_output_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "ff57876a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_split = 10000 / len(encoder_input_data)\n",
    "validation_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "9ec08021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "704/704 [==============================] - ETA: 0s - loss: 2.5578\n",
      "Epoch 1: val_loss improved from inf to 2.49781, saving model to 21_checkpoint.keras\n",
      "704/704 [==============================] - 150s 204ms/step - loss: 2.5578 - val_loss: 2.4978\n",
      "Epoch 2/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 2.2076\n",
      "Epoch 2: val_loss improved from 2.49781 to 2.33785, saving model to 21_checkpoint.keras\n",
      "704/704 [==============================] - 144s 204ms/step - loss: 2.2076 - val_loss: 2.3378\n",
      "Epoch 3/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 2.0469\n",
      "Epoch 3: val_loss improved from 2.33785 to 2.25086, saving model to 21_checkpoint.keras\n",
      "704/704 [==============================] - 143s 204ms/step - loss: 2.0468 - val_loss: 2.2509\n",
      "Epoch 4/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.9390\n",
      "Epoch 4: val_loss improved from 2.25086 to 2.20979, saving model to 21_checkpoint.keras\n",
      "704/704 [==============================] - 144s 205ms/step - loss: 1.9390 - val_loss: 2.2098\n",
      "Epoch 5/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.8567\n",
      "Epoch 5: val_loss improved from 2.20979 to 2.18043, saving model to 21_checkpoint.keras\n",
      "704/704 [==============================] - 143s 203ms/step - loss: 1.8566 - val_loss: 2.1804\n",
      "Epoch 6/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.7862\n",
      "Epoch 6: val_loss improved from 2.18043 to 2.16917, saving model to 21_checkpoint.keras\n",
      "704/704 [==============================] - 143s 203ms/step - loss: 1.7862 - val_loss: 2.1692\n",
      "Epoch 7/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.7265\n",
      "Epoch 7: val_loss improved from 2.16917 to 2.16432, saving model to 21_checkpoint.keras\n",
      "704/704 [==============================] - 143s 203ms/step - loss: 1.7265 - val_loss: 2.1643\n",
      "Epoch 8/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.6708\n",
      "Epoch 8: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 143s 203ms/step - loss: 1.6708 - val_loss: 2.1678\n",
      "Epoch 9/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.6211\n",
      "Epoch 9: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 143s 203ms/step - loss: 1.6211 - val_loss: 2.1686\n",
      "Epoch 10/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.5748\n",
      "Epoch 10: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.5748 - val_loss: 2.1741\n",
      "Epoch 11/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.5317\n",
      "Epoch 11: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.5317 - val_loss: 2.1923\n",
      "Epoch 12/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.4920\n",
      "Epoch 12: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.4920 - val_loss: 2.2009\n",
      "Epoch 13/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.4547\n",
      "Epoch 13: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.4548 - val_loss: 2.2148\n",
      "Epoch 14/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.4210\n",
      "Epoch 14: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.4211 - val_loss: 2.2415\n",
      "Epoch 15/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.3877\n",
      "Epoch 15: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.3877 - val_loss: 2.2507\n",
      "Epoch 16/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.3556\n",
      "Epoch 16: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.3556 - val_loss: 2.2544\n",
      "Epoch 17/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.3258\n",
      "Epoch 17: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.3258 - val_loss: 2.2771\n",
      "Epoch 18/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.2981\n",
      "Epoch 18: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.2983 - val_loss: 2.3008\n",
      "Epoch 19/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.2721\n",
      "Epoch 19: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.2722 - val_loss: 2.3136\n",
      "Epoch 20/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.2515\n",
      "Epoch 20: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.2515 - val_loss: 2.3295\n",
      "Epoch 21/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.2303\n",
      "Epoch 21: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.2302 - val_loss: 2.3354\n",
      "Epoch 22/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.2081\n",
      "Epoch 22: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.2082 - val_loss: 2.3541\n",
      "Epoch 23/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.1876\n",
      "Epoch 23: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.1876 - val_loss: 2.3664\n",
      "Epoch 24/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.1699\n",
      "Epoch 24: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.1699 - val_loss: 2.3804\n",
      "Epoch 25/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.1546\n",
      "Epoch 25: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.1545 - val_loss: 2.3893\n",
      "Epoch 26/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.1377\n",
      "Epoch 26: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.1377 - val_loss: 2.4110\n",
      "Epoch 27/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.1213\n",
      "Epoch 27: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.1213 - val_loss: 2.4201\n",
      "Epoch 28/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.1031\n",
      "Epoch 28: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.1031 - val_loss: 2.4292\n",
      "Epoch 29/1000\n",
      "704/704 [==============================] - ETA: 0s - loss: 1.0863\n",
      "Epoch 29: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.0863 - val_loss: 2.4357\n",
      "Epoch 30/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.0704\n",
      "Epoch 30: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.0705 - val_loss: 2.4493\n",
      "Epoch 31/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.0547\n",
      "Epoch 31: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.0547 - val_loss: 2.4469\n",
      "Epoch 32/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.0396\n",
      "Epoch 32: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.0397 - val_loss: 2.4645\n",
      "Epoch 33/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.0256\n",
      "Epoch 33: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.0255 - val_loss: 2.4691\n",
      "Epoch 34/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 1.0123\n",
      "Epoch 34: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 1.0123 - val_loss: 2.4862\n",
      "Epoch 35/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9992\n",
      "Epoch 35: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9992 - val_loss: 2.4972\n",
      "Epoch 36/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9871\n",
      "Epoch 36: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9872 - val_loss: 2.5063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9763\n",
      "Epoch 37: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9764 - val_loss: 2.5075\n",
      "Epoch 38/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9649\n",
      "Epoch 38: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9649 - val_loss: 2.5201\n",
      "Epoch 39/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9547\n",
      "Epoch 39: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9548 - val_loss: 2.5287\n",
      "Epoch 40/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9452\n",
      "Epoch 40: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9453 - val_loss: 2.5418\n",
      "Epoch 41/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9356\n",
      "Epoch 41: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9356 - val_loss: 2.5401\n",
      "Epoch 42/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9272\n",
      "Epoch 42: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9273 - val_loss: 2.5595\n",
      "Epoch 43/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9177\n",
      "Epoch 43: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9177 - val_loss: 2.5650\n",
      "Epoch 44/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9101\n",
      "Epoch 44: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9101 - val_loss: 2.5624\n",
      "Epoch 45/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.9028\n",
      "Epoch 45: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.9027 - val_loss: 2.5701\n",
      "Epoch 46/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8957\n",
      "Epoch 46: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8957 - val_loss: 2.5824\n",
      "Epoch 47/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8889\n",
      "Epoch 47: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8889 - val_loss: 2.5987\n",
      "Epoch 48/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8820\n",
      "Epoch 48: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8819 - val_loss: 2.5885\n",
      "Epoch 49/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8763\n",
      "Epoch 49: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8763 - val_loss: 2.5962\n",
      "Epoch 50/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8700\n",
      "Epoch 50: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8700 - val_loss: 2.6039\n",
      "Epoch 51/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8641\n",
      "Epoch 51: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8641 - val_loss: 2.6079\n",
      "Epoch 52/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8592\n",
      "Epoch 52: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8592 - val_loss: 2.6204\n",
      "Epoch 53/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8543\n",
      "Epoch 53: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8543 - val_loss: 2.6160\n",
      "Epoch 54/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8489\n",
      "Epoch 54: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8489 - val_loss: 2.6249\n",
      "Epoch 55/1000\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.8446\n",
      "Epoch 55: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8446 - val_loss: 2.6337\n",
      "Epoch 56/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8405\n",
      "Epoch 56: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8405 - val_loss: 2.6330\n",
      "Epoch 57/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8363\n",
      "Epoch 57: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8363 - val_loss: 2.6402\n",
      "Epoch 58/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8315\n",
      "Epoch 58: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8316 - val_loss: 2.6442\n",
      "Epoch 59/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8280\n",
      "Epoch 59: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8280 - val_loss: 2.6471\n",
      "Epoch 60/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8245\n",
      "Epoch 60: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8245 - val_loss: 2.6724\n",
      "Epoch 61/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8207\n",
      "Epoch 61: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8207 - val_loss: 2.6643\n",
      "Epoch 62/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8181\n",
      "Epoch 62: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8182 - val_loss: 2.6711\n",
      "Epoch 63/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8141\n",
      "Epoch 63: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8141 - val_loss: 2.6730\n",
      "Epoch 64/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8114\n",
      "Epoch 64: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8115 - val_loss: 2.6668\n",
      "Epoch 65/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8078\n",
      "Epoch 65: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8079 - val_loss: 2.6772\n",
      "Epoch 66/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8057\n",
      "Epoch 66: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8057 - val_loss: 2.6799\n",
      "Epoch 67/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8025\n",
      "Epoch 67: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8024 - val_loss: 2.6775\n",
      "Epoch 68/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.8004\n",
      "Epoch 68: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.8004 - val_loss: 2.6854\n",
      "Epoch 69/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7978\n",
      "Epoch 69: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7978 - val_loss: 2.6941\n",
      "Epoch 70/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7957\n",
      "Epoch 70: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7957 - val_loss: 2.6999\n",
      "Epoch 71/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7936\n",
      "Epoch 71: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7936 - val_loss: 2.6873\n",
      "Epoch 72/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7909\n",
      "Epoch 72: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7908 - val_loss: 2.7021\n",
      "Epoch 73/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7890\n",
      "Epoch 73: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7890 - val_loss: 2.7002\n",
      "Epoch 74/1000\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.7874\n",
      "Epoch 74: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7874 - val_loss: 2.7053\n",
      "Epoch 75/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7867\n",
      "Epoch 75: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7867 - val_loss: 2.7015\n",
      "Epoch 76/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7834\n",
      "Epoch 76: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7835 - val_loss: 2.7085\n",
      "Epoch 77/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7823\n",
      "Epoch 77: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7822 - val_loss: 2.7116\n",
      "Epoch 78/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7803\n",
      "Epoch 78: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7804 - val_loss: 2.7112\n",
      "Epoch 79/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7800\n",
      "Epoch 79: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7800 - val_loss: 2.7188\n",
      "Epoch 80/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7779\n",
      "Epoch 80: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7780 - val_loss: 2.7159\n",
      "Epoch 81/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7767\n",
      "Epoch 81: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7767 - val_loss: 2.7208\n",
      "Epoch 82/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7751\n",
      "Epoch 82: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7750 - val_loss: 2.7173\n",
      "Epoch 83/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7742\n",
      "Epoch 83: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7742 - val_loss: 2.7174\n",
      "Epoch 84/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7726\n",
      "Epoch 84: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7727 - val_loss: 2.7219\n",
      "Epoch 85/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7709\n",
      "Epoch 85: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7709 - val_loss: 2.7253\n",
      "Epoch 86/1000\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.7701\n",
      "Epoch 86: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7701 - val_loss: 2.7151\n",
      "Epoch 87/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7701\n",
      "Epoch 87: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7701 - val_loss: 2.7224\n",
      "Epoch 88/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7686\n",
      "Epoch 88: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7686 - val_loss: 2.7300\n",
      "Epoch 89/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7681\n",
      "Epoch 89: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7681 - val_loss: 2.7209\n",
      "Epoch 90/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7674\n",
      "Epoch 90: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7674 - val_loss: 2.7329\n",
      "Epoch 91/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7663\n",
      "Epoch 91: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7664 - val_loss: 2.7375\n",
      "Epoch 92/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7659\n",
      "Epoch 92: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7659 - val_loss: 2.7354\n",
      "Epoch 93/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7648\n",
      "Epoch 93: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7648 - val_loss: 2.7394\n",
      "Epoch 94/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7642\n",
      "Epoch 94: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7642 - val_loss: 2.7312\n",
      "Epoch 95/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7631\n",
      "Epoch 95: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7631 - val_loss: 2.7394\n",
      "Epoch 96/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7624\n",
      "Epoch 96: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7624 - val_loss: 2.7399\n",
      "Epoch 97/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7623\n",
      "Epoch 97: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7623 - val_loss: 2.7389\n",
      "Epoch 98/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7609\n",
      "Epoch 98: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7610 - val_loss: 2.7395\n",
      "Epoch 99/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7614\n",
      "Epoch 99: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7613 - val_loss: 2.7435\n",
      "Epoch 100/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7606\n",
      "Epoch 100: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7606 - val_loss: 2.7417\n",
      "Epoch 101/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7600\n",
      "Epoch 101: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7601 - val_loss: 2.7392\n",
      "Epoch 102/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7594\n",
      "Epoch 102: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7593 - val_loss: 2.7375\n",
      "Epoch 103/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7596\n",
      "Epoch 103: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7596 - val_loss: 2.7443\n",
      "Epoch 104/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7594\n",
      "Epoch 104: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7594 - val_loss: 2.7374\n",
      "Epoch 105/1000\n",
      "704/704 [==============================] - ETA: 0s - loss: 0.7582\n",
      "Epoch 105: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7582 - val_loss: 2.7463\n",
      "Epoch 106/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7577\n",
      "Epoch 106: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7577 - val_loss: 2.7436\n",
      "Epoch 107/1000\n",
      "703/704 [============================>.] - ETA: 0s - loss: 0.7583\n",
      "Epoch 107: val_loss did not improve from 2.16432\n",
      "704/704 [==============================] - 142s 202ms/step - loss: 0.7583 - val_loss: 2.7464\n",
      "Epoch 107: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x251bf118388>"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_train.fit(x=x_data,\n",
    "                y=y_data,\n",
    "                batch_size=128,\n",
    "                epochs=1000,\n",
    "                validation_split=validation_split,\n",
    "                callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "ea0626d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(input_text, true_output_text=None):\n",
    "    \"\"\"Translate a single text-string.\"\"\"\n",
    "\n",
    "    # Convert the input-text to integer-tokens.\n",
    "    # Note the sequence of tokens has to be reversed.\n",
    "    # Padding is probably not necessary.\n",
    "    input_tokens = tokenizer_src.text_to_tokens(text=input_text,\n",
    "                                                reverse=True,\n",
    "                                                padding=True)\n",
    "    \n",
    "    # Get the output of the encoder's GRU which will be\n",
    "    # used as the initial state in the decoder's GRU.\n",
    "    # This could also have been the encoder's final state\n",
    "    # but that is really only necessary if the encoder\n",
    "    # and decoder use the LSTM instead of GRU because\n",
    "    # the LSTM has two internal states.\n",
    "    initial_state = model_encoder.predict(input_tokens)\n",
    "\n",
    "    # Max number of tokens / words in the output sequence.\n",
    "    max_tokens = tokenizer_dest.max_tokens\n",
    "\n",
    "    # Pre-allocate the 2-dim array used as input to the decoder.\n",
    "    # This holds just a single sequence of integer-tokens,\n",
    "    # but the decoder-model expects a batch of sequences.\n",
    "    shape = (1, max_tokens)\n",
    "    decoder_input_data = np.zeros(shape=shape, dtype=np.int)\n",
    "\n",
    "    # The first input-token is the special start-token for 'ssss '.\n",
    "    token_int = token_start\n",
    "\n",
    "    # Initialize an empty output-text.\n",
    "    output_text = ''\n",
    "\n",
    "    # Initialize the number of tokens we have processed.\n",
    "    count_tokens = 0\n",
    "\n",
    "    # While we haven't sampled the special end-token for ' eeee'\n",
    "    # and we haven't processed the max number of tokens.\n",
    "    while token_int != token_end and count_tokens < max_tokens:\n",
    "        # Update the input-sequence to the decoder\n",
    "        # with the last token that was sampled.\n",
    "        # In the first iteration this will set the\n",
    "        # first element to the start-token.\n",
    "        decoder_input_data[0, count_tokens] = token_int\n",
    "\n",
    "        # Wrap the input-data in a dict for clarity and safety,\n",
    "        # so we are sure we input the data in the right order.\n",
    "        x_data = \\\n",
    "        {\n",
    "            'decoder_initial_state': initial_state,\n",
    "            'decoder_input': decoder_input_data\n",
    "        }\n",
    "\n",
    "        # Note that we input the entire sequence of tokens\n",
    "        # to the decoder. This wastes a lot of computation\n",
    "        # because we are only interested in the last input\n",
    "        # and output. We could modify the code to return\n",
    "        # the GRU-states when calling predict() and then\n",
    "        # feeding these GRU-states as well the next time\n",
    "        # we call predict(), but it would make the code\n",
    "        # much more complicated.\n",
    "\n",
    "        # Input this data to the decoder and get the predicted output.\n",
    "        decoder_output = model_decoder.predict(x_data)\n",
    "\n",
    "        # Get the last predicted token as a one-hot encoded array.\n",
    "        token_onehot = decoder_output[0, count_tokens, :]\n",
    "        \n",
    "        # Convert to an integer-token.\n",
    "        token_int = np.argmax(token_onehot)\n",
    "\n",
    "        # Lookup the word corresponding to this integer-token.\n",
    "        sampled_word = tokenizer_dest.token_to_word(token_int)\n",
    "\n",
    "        # Append the word to the output-text.\n",
    "        output_text += \" \" + sampled_word\n",
    "\n",
    "        # Increment the token-counter.\n",
    "        count_tokens += 1\n",
    "\n",
    "    # Sequence of tokens output by the decoder.\n",
    "    output_tokens = decoder_input_data[0]\n",
    "    \n",
    "    # Print the input-text.\n",
    "    print(\"Input text:\")\n",
    "    print(input_text)\n",
    "    print()\n",
    "\n",
    "    # Print the translated output-text.\n",
    "    print(\"Translated text:\")\n",
    "    print(output_text)\n",
    "    print()\n",
    "\n",
    "    # Optionally print the true translated text.\n",
    "    if true_output_text is not None:\n",
    "        print(\"True output text:\")\n",
    "        print(true_output_text)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d0cbd4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tammaa\\anaconda3\\envs\\env1\\lib\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "ssss You have requested a debate on this subject in the course of the next few days, during this part-session. eeee\n",
      "\n",
      "Translated text:\n",
      " in einer zeit in diesem plenum ist ein thema in der konferenz in der region                                                                  \n",
      "\n",
      "True output text:\n",
      "Im Parlament besteht der Wunsch nach einer Aussprache im Verlauf dieser Sitzungsperiode in den nächsten Tagen.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 3\n",
    "translate(input_text=qs[:,0][idx],\n",
    "          true_output_text=qs[:,1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "adafb215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tammaa\\anaconda3\\envs\\env1\\lib\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "ssss Mr Hänsch represented you on this occasion. eeee\n",
      "\n",
      "Translated text:\n",
      " herr solana haben sie sich mit den                                                                                  \n",
      "\n",
      "True output text:\n",
      "Der Kollege Hänsch hat Sie dort vertreten.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx = 90\n",
    "translate(input_text=qs[:,0][idx],\n",
    "          true_output_text=qs[:,1][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "b868fa4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tammaa\\anaconda3\\envs\\env1\\lib\\site-packages\\ipykernel_launcher.py:26: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "The term political statement is used to refer to any act or non-verbal form of communication that is intended to influence a decision to be made for or by a political party. A political statement can vary from a mass demonstration to the wearing of a badge with a political slogan\n",
      "\n",
      "Translated text:\n",
      " deshalb ist eine politische entscheidung über eine politische entscheidung oder einer entscheidung zu einer entscheidung zu einer entscheidung zu einer entscheidung zu einer entscheidung zu einer entscheidung zu einer entscheidung zu einer entscheidung zu einer entscheidung zu einer entscheidung zu einer entscheidung zu einer entscheidung zu einer entscheidung\n",
      "\n",
      "True output text:\n",
      "Ich möchte der Europäischen Union beitreten\n",
      "\n"
     ]
    }
   ],
   "source": [
    "translate(input_text=\"The term political statement is used to refer to any act or non-verbal form of communication that is intended to influence a decision to be made for or by a political party. A political statement can vary from a mass demonstration to the wearing of a badge with a political slogan\",\n",
    "          true_output_text='Ich möchte der Europäischen Union beitreten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a701b31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3db9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e8cd62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1008d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0f2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cb53a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccc1ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c3a8c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fad5433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba5eef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf63147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c5d454",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e12685",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd30a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649e5120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc13921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbfc0af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3342a0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc7d725",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c8c85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2925de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b911dd21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b905df7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1767b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f4be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f86de49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b391bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1baf267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54d51ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c98f24a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f7336f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f176f9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e912d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f2eac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5597b25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838c89e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269262fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aaf4799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c108aa86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e0c89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee919e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b3a530",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2421ec35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76090504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba445a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca79a00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61245a5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38c978",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9320beaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7313ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs = np.array(qs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb806a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "qs[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dbfd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_tokenizer = tokenization(`)\n",
    "de_tokenizer = tokenization(qs[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922a197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index)+1\n",
    "eng_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0b240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_vocab_size = len(de_tokenizer.word_index)+1\n",
    "de_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5035b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff64ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length =  max([len(x) for x in qs[:,0]])\n",
    "max_decoder_seq_length = max([len(x) for x in qs[:,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede0dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max_encoder_seq_length, max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2016042",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_sequences(tokenizer, length, lines):\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    seq = pad_sequences(seq,maxlen=length,padding='post')\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae96e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ffdf78",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(qs,test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05eb6cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9632d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_length = max_encoder_seq_length\n",
    "de_length = max_decoder_seq_length\n",
    "units= 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f91345",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = encode_sequences(de_tokenizer,en_length, train[:,1])\n",
    "train_y = encode_sequences(eng_tokenizer,de_length,train[:,0])\n",
    "\n",
    "test_x = encode_sequences(de_tokenizer,en_length, test[:,1])\n",
    "test_y = encode_sequences(eng_tokenizer,de_length,test[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb03875",
   "metadata": {},
   "source": [
    "# Define The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c230e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "units=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d25ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b803d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(32,units,input_length=in_timesteps, mask_zero=True))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    \n",
    "    model.add(Dense(32,activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c124f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = define_model(eng_vocab_size,de_vocab_size,en_length,de_length,units)\n",
    "rms = RMSprop(learning_rate=0.001)\n",
    "model.compile(optimizer=rms,loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5dd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05b2f7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_x,train_y.reshape((train_y.shape[0],train_y.shape[1],1)),\n",
    "                   epochs=50,\n",
    "                    batch_size=512\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c73a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model,to_file=\"translation.png\",show_shapes=True,dpi=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17566cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index ==n:\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ae16ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(test_x.reshape((test_x.shape[0],test_x.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a949c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd61105",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j>0:\n",
    "            if (t== get_word(i[j-1], eng_tokenizer) or (t==None)):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "        else:\n",
    "            if(t==None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bed0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c458aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
